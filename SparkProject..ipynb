{
  "metadata": {
    "name": "SparkProject",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n1+1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd \u003d sc.textFile(\u0027s3://megadados-alunos/dados/all_reviews_clean_tsv/\u0027).cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d spark.read.option(\"header\", \"false\") \\\n    .option(\"delimiter\", \"\\t\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"s3://megadados-alunos/dados/all_reviews_clean_tsv/\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d df \\\n    .withColumnRenamed(\"_c0\", \"marketplace\") \\\n    .withColumnRenamed(\"_c1\", \"customer_id\") \\\n    .withColumnRenamed(\"_c2\", \"review_id\") \\\n    .withColumnRenamed(\"_c3\", \"product_id\") \\\n    .withColumnRenamed(\"_c4\", \"product_parent\") \\\n    .withColumnRenamed(\"_c5\", \"product_title\") \\\n    .withColumnRenamed(\"_c6\", \"product_category\") \\\n    .withColumnRenamed(\"_c7\", \"star_rating\") \\\n    .withColumnRenamed(\"_c8\", \"helpful_votes\") \\\n    .withColumnRenamed(\"_c9\", \"total_votes\") \\\n    .withColumnRenamed(\"_c10\", \"vine\") \\\n    .withColumnRenamed(\"_c11\", \"verified_purchase\") \\\n    .withColumnRenamed(\"_c12\", \"review_headline\") \\\n    .withColumnRenamed(\"_c13\", \"review_body\") \\\n    .withColumnRenamed(\"_c14\", \"review_date\") "
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d df.cache()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Task 1\n• Quantos reviews existem? R: n_reviews\n• Quantos clientes existem? R: n_clientes\n• Quantos produtos existem? R: n_produtos\n• Quantos reviews existem para cada “star_rating” (de 1 a 5 estrelas)? R: n_reviews[1:6]"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Item 1\nn_reviews \u003d df[[\"review_id\"]].distinct().count()\nprint(f\"Número de reviews \u003d {n_reviews}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Item 2\nn_clientes \u003d df[[\"customer_id\"]].distinct().count()\nprint(f\"Número de clientes \u003d {n_clientes}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Item 3\nn_produtos \u003d df[[\"product_id\"]].distinct().count()\nprint(f\"Número de produtos \u003d {n_produtos}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\nn_reviews1 \u003d df.filter(df.star_rating \u003d\u003d 1).count()\nn_reviews2 \u003d df.filter(df.star_rating \u003d\u003d 2).count()\nn_reviews3 \u003d df.filter(df.star_rating \u003d\u003d 3).count()\nn_reviews4 \u003d df.filter(df.star_rating \u003d\u003d 4).count()\nn_reviews5 \u003d df.filter(df.star_rating \u003d\u003d 5).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Item 4\nprint(f\"Número de reviews com 1 estrela \u003d {n_reviews1}\\nNúmero de reviews com 2 estrelas \u003d {n_reviews2}\\nNúmero de reviews com 3 estrelas \u003d {n_reviews3}\\nNúmero de reviews com 3 estrelas \u003d {n_reviews3}\\nNúmero de reviews com 4 estrelas \u003d {n_reviews4}\\nNúmero de reviews com 5 estrelas \u003d {n_reviews5}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Item 5\ndf_single_customer \u003d df.groupBy(\"customer_id\").count()\ndf_single_customer.show(n\u003d10)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_single_customer \u003d df_single_customer.filter(\"count \u003d\u003d 1\")\n#df_single_customer.show(n\u003d10)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_single_customer_full \u003d df.join(df_single_customer, [\"customer_id\"])\n#df_single_customer_full.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nproducts_above_10_ratings \u003d df_single_customer_full.where((df[\"star_rating\"] \u003d\u003d \u00271\u0027) | (df[\"star_rating\"] \u003d\u003d \u00272\u0027) | (df[\"star_rating\"] \u003d\u003d \u00273\u0027) | (df[\"star_rating\"] \u003d\u003d \u00274\u0027) | (df[\"star_rating\"] \u003d\u003d \u00275\u0027))\nproducts_above_10_ratings.show()\n                                                        \n                                                        \n                                                        \n                                                        \n                                                        \n                                                        \n                                                        "
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\np10 \u003d products_above_10_ratings.groupBy(\"product_id\").count()\np10 \u003d p10.filter(\"count \u003e 10\")\np10_full \u003d df.join(p10, [\"product_id\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import IntegerType\n#p10_full \u003d p10_full.withColumn(\"star_rating\", p10_full[\"star_rating\"].cast(IntegerType()))\np10_full.describe([\"count\"]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\np10_mean \u003d 244 #243.4565352161903\ntop10 \u003d p10_full.filter(\"count \u003e 244\")\ntop10.orderBy([\"count\"],ascending\u003dFalse).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop10_names \u003d top10.orderBy([\"count\"],ascending\u003dFalse) #.select(\"product_id\",\"product_title\").distinct().limit(10).show()\ntop10_names.select(\"product_id\",\"product_title\",\"count\").distinct().show(n\u003d10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Task 2"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_customer \u003d df.groupBy(\"customer_id\").count()\ndf_customer.describe([\"count\"]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncriterio \u003d round(4.506656831142033 + 20.13220742073738) # Média + desvio padrão\nbots_customer \u003d df_customer.where(df_customer[\"count\"] \u003e criterio)\nreal_customer \u003d df_customer.where(df_customer[\"count\"] \u003c\u003d criterio)\nbots_customer.describe([\"count\"]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\njoined \u003d df.join(df_customer,[\"customer_id\"])\nbots_full \u003d joined.where(joined[\"count\"] \u003e criterio)\n#bots_full \u003d df.join(bots_customer, df.customer_id \u003d\u003d bots_customer.customer_id)\nbots_full.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbots_full.describe([\"count\"]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbots_full[[\"customer_id\",\"product_category\",\"star_rating\"]]"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbots_full[[\"customer_id\"]].distinct().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbots_categories \u003d bots_full.cube(\"product_category\").count()\nbots_categories.orderBy([\"count\"], ascending\u003dFalse).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbots_ratings \u003d bots_full.cube(\"star_rating\").count()\nbots_ratings.orderBy([\"count\"], ascending\u003dFalse).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Task 3\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import when\ndf_wo_nulls \u003d df.na.drop()\ndf3 \u003d df_wo_nulls.withColumn(\"label_rating\", when(df[\"star_rating\"]\u003d\u003d1, \"negative\")\n                                                .when(df[\"star_rating\"]\u003d\u003d2, \"negative\")\n                                                .when(df[\"star_rating\"]\u003d\u003d3, \"negative\")\n                                                .when(df[\"star_rating\"]\u003d\u003d4, \"neutral\")\n                                                .when(df[\"star_rating\"]\u003d\u003d5, \"POSITIVE\"))\ndf3.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Import the required packages\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import NaiveBayes\n\nstages \u003d []\n# 1. clean data and tokenize sentences using RegexTokenizer\nregexTokenizer \u003d RegexTokenizer(inputCol\u003d\"review_body\", outputCol\u003d\"tokens\", pattern\u003d\"\\\\W+\")\nstages +\u003d [regexTokenizer]\n\n# 2. CountVectorize the data\ncv \u003d CountVectorizer(inputCol\u003d\"tokens\", outputCol\u003d\"token_features\", minDF\u003d2.0)#, vocabSize\u003d3, minDF\u003d2.0\nstages +\u003d [cv]\n\n# 3. Convert the labels to numerical values using binariser\nindexer \u003d StringIndexer(inputCol\u003d\"label_rating\", outputCol\u003d\"label\")\nstages +\u003d [indexer]\n\n# 4. Vectorise features using vectorassembler\nvecAssembler \u003d VectorAssembler(inputCols\u003d[\u0027token_features\u0027], outputCol\u003d\"features\")\nstages +\u003d [vecAssembler]\n\n[print(\u0027\\n\u0027, stage) for stage in stages]"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import Pipeline\npipeline \u003d Pipeline(stages\u003dstages)\ndata \u003d pipeline.fit(df3).transform(df3)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain, test \u003d data.randomSplit([0.7, 0.3], seed \u003d 2018)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import NaiveBayes\n# Initialise the model\nnb \u003d NaiveBayes(smoothing\u003d1.0, modelType\u003d\"multinomial\")\n# Fit the model\nmodel \u003d nb.fit(train)\n# Make predictions on test data\npredictions \u003d model.transform(test)\npredictions.select(\"label\", \"prediction\", \"probability\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator \u003d BinaryClassificationEvaluator(rawPredictionCol\u003d\"prediction\")\naccuracy \u003d evaluator.evaluate(predictions)\nprint (\"Model Accuracy: \", accuracy)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}